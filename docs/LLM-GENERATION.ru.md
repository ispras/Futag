# Генерация фаззинг-оберток с использованием LLM

## Обзор

Futag теперь поддерживает генерацию фаззинг-оберток с использованием больших языковых моделей (LLM), аналогично проекту [oss-fuzz-gen](https://github.com/google/oss-fuzz-gen). Эта функция использует передовые языковые модели для автоматической генерации качественных фаззинг-оберток.

## Основные возможности

- **Несколько провайдеров LLM**: Поддержка OpenAI (GPT-4, GPT-3.5-turbo), Anthropic (Claude) и локальных моделей
- **Умная генерация кода**: Использует продвинутые промпты для генерации качественных фаззинг-оберток
- **Гибкая интеграция**: Может использоваться отдельно или в комбинации с традиционным статическим анализом
- **Настраиваемость**: Регулируемые параметры temperature, max tokens и выбор модели

## Установка

Установите необходимые зависимости:

```bash
pip install openai anthropic
```

Или используйте файл requirements:

```bash
cd src/python/futag-package
pip install -r requirements.txt
```

## Быстрый старт

### 1. Настройка API ключа

Установите API ключ как переменную окружения:

```bash
# Для OpenAI
export OPENAI_API_KEY="ваш-ключ-openai"

# Для Anthropic
export ANTHROPIC_API_KEY="ваш-ключ-anthropic"
```

### 2. Базовое использование

```python
from futag.preprocessor import Builder
from futag.generator import Generator

# Сборка и анализ библиотеки
builder = Builder("futag-llvm/", "путь/к/библиотеке/")
builder.auto_build()
builder.analyze()

# Генерация фаззинг-оберток с помощью LLM
generator = Generator("futag-llvm/", "путь/к/библиотеке/")
stats = generator.gen_targets_with_llm(
    llm_provider="openai",
    llm_model="gpt-4",
    max_functions=10
)

print(f"Сгенерировано {stats['successful']} фаззинг-оберток")
```

## Продвинутое использование

### Использование различных провайдеров LLM

#### OpenAI GPT-4

```python
stats = generator.gen_targets_with_llm(
    llm_provider="openai",
    llm_model="gpt-4",
    llm_api_key="ваш-ключ",  # Опционально, если установлена переменная окружения
    max_functions=10,
    temperature=0.7,
    max_tokens=2048
)
```

#### OpenAI GPT-3.5-turbo (Быстрее/Дешевле)

```python
stats = generator.gen_targets_with_llm(
    llm_provider="openai",
    llm_model="gpt-3.5-turbo",
    max_functions=20,
    temperature=0.5
)
```

#### Anthropic Claude

```python
stats = generator.gen_targets_with_llm(
    llm_provider="anthropic",
    llm_model="claude-3-opus-20240229",
    max_functions=10
)
```

### Гибридный подход: Традиционный + LLM

Комбинируйте традиционный статический анализ с LLM-генерацией:

```python
from futag.generator import Generator

generator = Generator("futag-llvm/", "путь/к/библиотеке/")

# Сначала: Традиционная генерация
generator.gen_targets(anonymous=False, max_wrappers=10)

# Затем: Дополнение LLM-генерацией
llm_stats = generator.gen_targets_with_llm(
    llm_provider="openai",
    llm_model="gpt-4",
    max_functions=5
)

# Компиляция всех целей
generator.compile_targets(workers=4, keep_failed=True)
```

## Параметры конфигурации

### Параметры `gen_targets_with_llm()`

| Параметр | Тип | По умолчанию | Описание |
|----------|-----|--------------|----------|
| `llm_provider` | str | "openai" | Провайдер LLM: 'openai', 'anthropic' или 'local' |
| `llm_model` | str | "gpt-4" | Название модели (например, 'gpt-4', 'gpt-3.5-turbo', 'claude-3-opus-20240229') |
| `llm_api_key` | str | None | API ключ (или использовать переменную окружения) |
| `max_functions` | int | None | Максимальное количество функций для генерации (None = все) |
| `temperature` | float | 0.7 | Temperature LLM (0.0-1.0, меньше = более детерминированный) |
| `max_tokens` | int | 2048 | Максимальное количество токенов в ответе LLM |

### Руководство по temperature

- **0.0-0.3**: Высокая детерминированность, консервативная генерация
- **0.4-0.7**: Сбалансированная креативность и консистентность (рекомендуется)
- **0.8-1.0**: Более креативно, но потенциально менее надежно

## Результаты

Сгенерированные обертки сохраняются в:
- `futag-fuzz-drivers/<имя_функции>/<имя_функции>_llm_fuzz.c`
- Статистика: `futag-fuzz-drivers/llm_generation_stats.json`

Пример файла статистики:
```json
{
  "total": 10,
  "successful": 8,
  "failed": 2,
  "successful_functions": ["func1", "func2", ...],
  "failed_functions": ["func3", "func4"]
}
```

## Сравнение: Традиционный vs LLM-подход

### Традиционный статический анализ

**Преимущества:**
- Без затрат на API
- Быстрый и детерминированный
- Работает оффлайн
- Хорошо протестирован и надежен

**Недостатки:**
- Может испытывать трудности со сложными типами
- Ограничен предопределенными шаблонами
- Менее гибкий

### LLM-генерация

**Преимущества:**
- Лучше справляется со сложными сценариями
- Более гибкий и адаптивный
- Может учиться на контексте
- Похож на написанные человеком обертки

**Недостатки:**
- Требует доступа к API и стоит денег
- Недетерминированный (варьируется между запусками)
- Требуется интернет-соединение
- Сгенерированный код следует проверять

### Рекомендация

Используйте **гибридный подход**:
1. Начните с традиционной генерации для стандартных случаев
2. Используйте LLM для сложных или проблемных функций
3. Проверяйте и тестируйте все сгенерированные обертки

## Оценка стоимости

Приблизительные цены (на 2024 год):

| Модель | Стоимость за 1M токенов | Примерно на функцию |
|--------|------------------------|---------------------|
| GPT-4 | $30 (вход) / $60 (выход) | $0.05-$0.15 |
| GPT-3.5-turbo | $0.50 (вход) / $1.50 (выход) | $0.01-$0.03 |
| Claude-3-Opus | $15 (вход) / $75 (выход) | $0.03-$0.10 |

**Совет**: Начните с GPT-3.5-turbo для тестирования, затем используйте GPT-4 для продакшена.

## Примеры

Смотрите полные примеры в:
- `src/python/example-llm-generation.py` - Всеобъемлющий пример со всеми функциями
- `examples/` - Примеры сгенерированных оберток

## Устранение неполадок

### Проблемы с API ключом

```python
# Проверить, установлены ли зависимости
from futag.llm_generator import check_llm_dependencies
if check_llm_dependencies("openai"):
    print("Зависимости OpenAI доступны")
```

### Распространенные ошибки

1. **ImportError: No module named 'openai'**
   - Решение: `pip install openai`

2. **API ключ не найден**
   - Решение: Установите переменную окружения или передайте параметр `llm_api_key`

3. **Ошибки ограничения скорости**
   - Решение: Используйте `max_functions` для ограничения запросов, добавьте задержки между вызовами

## Будущие улучшения

Планируемые функции:
- [ ] Поддержка локальных LLM (Ollama, LM Studio)
- [ ] Тонко настроенные модели для фаззинга
- [ ] Итеративное улучшение на основе результатов компиляции
- [ ] Интеграция с результатами фаззинга для обратной связи
- [ ] Оптимизация затрат и кеширование

## Ссылки

- [oss-fuzz-gen](https://github.com/google/oss-fuzz-gen) - Инструмент фаззинга на основе LLM от Google
- [Документация OpenAI API](https://platform.openai.com/docs)
- [Документация Anthropic API](https://docs.anthropic.com/)
- [Документация LibFuzzer](https://llvm.org/docs/LibFuzzer.html)

## Участие в разработке

Чтобы внести вклад в LLM-генерацию:
1. Тестируйте с различными моделями LLM
2. Улучшайте промпты в `llm_generator.py`
3. Добавляйте поддержку новых провайдеров LLM
4. Делитесь своими результатами и отзывами
